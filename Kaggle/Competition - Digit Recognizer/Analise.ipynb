{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1d5e6b-f068-49b7-9f83-a2dc4a575038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a97d8ca-08c0-4295-a09a-50dfad6694e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8e76b0-c089-48a0-abd0-8b06becde09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dados do CSV \"train.csv\"\n",
    "train_data = np.loadtxt(\"./train.csv\", delimiter=\",\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fea6da-bd99-4367-8fe1-484eeeef3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 42000\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de dados de treino\n",
    "print('images:', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f35d8a-7b73-4d7e-b78a-821cf5bda753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa labels das informações da imagem\n",
    "Y_train = train_data[:, 0].astype(int)   # labels (dígitos 0–9)\n",
    "X_train = train_data[:, 1:]              # pixels (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73880133-c605-40c7-8741-431a5d5c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altera valores da image de (0 à 255) para (0.0 à 1.0)\n",
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3470698-78e8-4dad-a821-ebf870a4079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de rede neural\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88015003-9590-43d0-866f-30dba1338ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila o modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d0e559-e267-4b66-ab9c-67ce8f6b5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aruma formato dos dados das imagens para ser consumido pelo TensorFlow\n",
    "X_train = X_train.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae8f0cf-a437-4db0-9a61-12c145342c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1050/1050 [==============================] - 3s 2ms/step - loss: 0.3177 - accuracy: 0.9060 - val_loss: 0.1656 - val_accuracy: 0.9493\n",
      "Epoch 2/5\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.1313 - accuracy: 0.9601 - val_loss: 0.1351 - val_accuracy: 0.9587\n",
      "Epoch 3/5\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0887 - accuracy: 0.9726 - val_loss: 0.1047 - val_accuracy: 0.9676\n",
      "Epoch 4/5\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0647 - accuracy: 0.9801 - val_loss: 0.1060 - val_accuracy: 0.9688\n",
      "Epoch 5/5\n",
      "1050/1050 [==============================] - 2s 2ms/step - loss: 0.0492 - accuracy: 0.9849 - val_loss: 0.1350 - val_accuracy: 0.9606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x280b6667280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treina o modelo\n",
    "model.fit(X_train, Y_train, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99e071e-632e-4c3d-bb10-d853ae14d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 1s 933us/step\n",
      "Acurácia: 97.49285714285715 %\n"
     ]
    }
   ],
   "source": [
    "# Faz as previsões do \"x_text\"\n",
    "predictions = model.predict(X_train)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Comparar predições com os valores reais\n",
    "accuracy = np.mean(predicted_classes == Y_train)\n",
    "\n",
    "print(\"Acurácia:\", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dc3167d-f79f-479e-8dcd-743703c8b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dados para teste\n",
    "test_data = np.loadtxt(\"./test.csv\", delimiter=\",\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae0ec08-6ede-4c75-9110-01c08ddd9a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 28000\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de dados de teste\n",
    "print('images:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a927d1d8-1053-4e84-8c85-615c3f5a8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altera valores da image de (0 à 155) para (0.0 à 1.0)\n",
    "X_test = test_data / 255.0\n",
    "\n",
    "# Aruma formato dos dados das imagens para ser consumido pelo TensorFlow\n",
    "X_test = X_test.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4a24224-2583-4fb9-8326-2171a4ba6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Faz as previsões do \"x_text\"\n",
    "predictions_test = model.predict(X_test)\n",
    "predicted_classes_test = np.argmax(predictions_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4f167b-6d38-4c2f-b867-31720e9749ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras 10 previsões:\n",
      "Imagem 0: Previsto = 2\n",
      "Imagem 1: Previsto = 0\n",
      "Imagem 2: Previsto = 9\n",
      "Imagem 3: Previsto = 0\n",
      "Imagem 4: Previsto = 3\n",
      "Imagem 5: Previsto = 7\n",
      "Imagem 6: Previsto = 0\n",
      "Imagem 7: Previsto = 3\n",
      "Imagem 8: Previsto = 0\n",
      "Imagem 9: Previsto = 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeiras 10 previsões:\")\n",
    "for i in range(10):\n",
    "    print(f\"Imagem {i}: Previsto = {predicted_classes_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29eab971-8c27-4bbc-9d8c-742f8139ad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pega a primeira imagem\n",
    "first_image = test_data[0].reshape(28, 28)\n",
    "\n",
    "# Mostra a imagem com número preto e fundo branco\n",
    "plt.imshow(first_image, cmap=\"gray_r\")\n",
    "plt.axis(\"off\")  # remove os eixos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d983451-5e35-459b-a862-c3dce9b9d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria coluna ImageId (começa em 1 até N)\n",
    "image_ids = np.arange(1, len(predicted_classes_test) + 1)\n",
    "\n",
    "# Junta em um DataFrame\n",
    "output = pd.DataFrame({\n",
    "    \"ImageId\": image_ids,\n",
    "    \"Label\": predicted_classes_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8eb367-c0ad-4d20-9c92-96a0c14bc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva no CSV (sem índice extra)\n",
    "output.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
